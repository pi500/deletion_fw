{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "workspace7971"
		},
		"AzureBlobStorage1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureBlobStorage1'"
		},
		"AzureDataLakeStorage1_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1'"
		},
		"workspace7971-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'workspace7971-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:workspace7971.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"AzureDataLakeStorage1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://myadls8434.dfs.core.windows.net/"
		},
		"workspace7971-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://myadls8434.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 3')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Get Metadata1",
						"type": "GetMetadata",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Json2",
								"type": "DatasetReference",
								"parameters": {}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "JsonReadSettings"
							}
						}
					},
					{
						"name": "ForEach1",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get Metadata1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get Metadata1').output.childItems",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Lookup1",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "JsonSource",
											"storeSettings": {
												"type": "AzureBlobFSReadSettings",
												"recursive": true,
												"wildcardFileName": {
													"value": "@item().name",
													"type": "Expression"
												},
												"enablePartitionDiscovery": false
											},
											"formatSettings": {
												"type": "JsonReadSettings"
											}
										},
										"dataset": {
											"referenceName": "Json3",
											"type": "DatasetReference",
											"parameters": {}
										},
										"firstRowOnly": false
									}
								}
							]
						}
					},
					{
						"name": "Lookup2",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "JsonSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"wildcardFileName": "*.json",
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "JsonReadSettings"
								}
							},
							"dataset": {
								"referenceName": "Json3",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "Set variable1",
						"type": "SetVariable",
						"dependsOn": [],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "config_names",
							"value": {
								"value": "@pipeline().parameters.config_names",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Get Metadata_new",
						"type": "GetMetadata",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Json2",
								"type": "DatasetReference",
								"parameters": {}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "JsonReadSettings"
							}
						}
					},
					{
						"name": "ForEach2",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get Metadata_new",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get Metadata_new').output.childItems",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "my_notebook",
									"type": "SynapseNotebook",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"notebook": {
											"referenceName": "Notebook 2",
											"type": "NotebookReference"
										},
										"parameters": {
											"file_name": {
												"value": {
													"value": "@item().name",
													"type": "Expression"
												},
												"type": "string"
											}
										},
										"snapshot": true,
										"sparkPool": {
											"referenceName": "mypool",
											"type": "BigDataPoolReference"
										},
										"executorSize": "Small",
										"conf": {},
										"driverSize": "Small"
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"config_names": {
						"type": "array",
						"defaultValue": [
							"data_master.json",
							"delete_master.json"
						]
					},
					"data": {
						"type": "string"
					}
				},
				"variables": {
					"config_names": {
						"type": "Array"
					},
					"data": {
						"type": "String"
					}
				},
				"annotations": [],
				"lastPublishTime": "2024-01-05T15:48:25Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Json2')]",
				"[concat(variables('workspaceId'), '/datasets/Json3')]",
				"[concat(variables('workspaceId'), '/notebooks/Notebook 2')]",
				"[concat(variables('workspaceId'), '/bigDataPools/mypool')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Json1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureBlobStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "data_master.json",
						"folderPath": "my_dict",
						"container": "historical"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"data_node": {
							"type": "string"
						},
						"data_product": {
							"type": "string"
						},
						"data_table": {
							"type": "string"
						},
						"record_type": {
							"type": "string"
						},
						"container": {
							"type": "string"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureBlobStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Json2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "new",
						"fileSystem": "historical"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"even_id": {
							"type": "string"
						},
						"record_type": {
							"type": "string"
						},
						"record_id": {
							"type": "string"
						},
						"date": {
							"type": "string"
						},
						"status": {
							"type": "string"
						},
						"data_node": {
							"type": "string"
						},
						"data_product": {
							"type": "string"
						},
						"data_table": {
							"type": "string"
						},
						"container": {
							"type": "string"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Json3')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "new",
						"fileSystem": "historical"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"data_node": {
							"type": "string"
						},
						"data_product": {
							"type": "string"
						},
						"data_table": {
							"type": "string"
						},
						"record_type": {
							"type": "string"
						},
						"container": {
							"type": "string"
						},
						"even_id": {
							"type": "string"
						},
						"record_id": {
							"type": "string"
						},
						"date": {
							"type": "string"
						},
						"status": {
							"type": "string"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureBlobStorage1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/workspace7971-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('workspace7971-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/workspace7971-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('workspace7971-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "mypool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "652bb34e-ab2f-43ea-893c-60bcb355a90f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7374728e-d82a-4420-8e42-c2f7ef16e4c3/resourceGroups/mygrp/providers/Microsoft.Synapse/workspaces/workspace7971/bigDataPools/mypool",
						"name": "mypool",
						"type": "Spark",
						"endpoint": "https://workspace7971.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/mypool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"#loc = f'abfss://historical@myadls8434/new/{file_name}'\r\n",
							"loc = f'abfss://historical@myadls8434/new/data_master.json'\r\n",
							"print(loc)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"adls_path = \"adl://myadls8434.azuredatalakestore.net/historical/new/your/data_master.json\"\r\n",
							"#loc = f'abfss://historical@myadls8434/new/data_master.json'\r\n",
							"# Read JSON file into a DataFrame\r\n",
							"df = spark.read.json(adls_path)\r\n",
							"\r\n",
							"# Show DataFrame schema and some sample data\r\n",
							"df.printSchema()\r\n",
							"df.show()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Import necessary libraries\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"# Create a Spark session\r\n",
							"\r\n",
							"\r\n",
							"# Define the path to your JSON file in Azure Storage\r\n",
							"file_path = \"abfss://historical@myadls8434.dfs.core.windows.net/new/delete_master.json\"\r\n",
							"\r\n",
							"# Read JSON file into DataFrame\r\n",
							"df = spark.read.json(file_path)\r\n",
							"\r\n",
							"# Show DataFrame schema and some data\r\n",
							"df.printSchema()\r\n",
							"df.show()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.types import *\r\n",
							"schema1 = StructType([\\\r\n",
							"    StructField('data_node',StringType()),\\\r\n",
							"    StructField('data_product',StringType()),\\\r\n",
							"    StructField('data_table',StringType()),\\\r\n",
							"    StructField('record_type',StringType()),\\\r\n",
							"    StructField('container',StringType())])\r\n",
							"file_path = \"abfss://historical@myadls8434.dfs.core.windows.net/new/data_master.json\"\r\n",
							"data = [\r\n",
							"\t{\r\n",
							"\"data_node\" : \"IPN\",\r\n",
							"\"data_product\" : \"InvolvedParty\",\r\n",
							"\"data_table\" : \"PERSON\",\r\n",
							"\"record_type\" : \"Customer\",\r\n",
							"\"container\" : \"historical\"\r\n",
							"}\r\n",
							"]\r\n",
							"# Read JSON file into DataFrame\r\n",
							"#df = spark.read.json(file_path)\r\n",
							"df= spark.createDataFrame(data,schema1)\r\n",
							"\r\n",
							"# Show DataFrame schema and some data\r\n",
							"df.printSchema()\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"file_path = \"abfss://historical@myadls8434.dfs.core.windows.net/new/data_master.json\"\r\n",
							"# Read JSON file into an RDD\r\n",
							"json_rdd = sc.textFile(file_path)\r\n",
							"\r\n",
							"# Show some content from the RDD\r\n",
							"json_rdd.collect()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 89
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"son_rdd = json_rdd.map(lambda x: json.dumps(x))\r\n",
							"\r\n",
							"# Combine JSON strings into a single JSON object or array\r\n",
							"# If an array is needed:\r\n",
							"json_array = \"[\" + ','.join(json_rdd.collect()) + \"]\"\r\n",
							"#print(json_array)\r\n",
							"modified_string = json_array[4:-4]\r\n",
							"modified_string = modified_string.replace(',},,{,', '}{')\r\n",
							"modified_string = modified_string.replace(',,', ',')\r\n",
							"#modified_string = '{'+modified_string+'}'\r\n",
							"print(modified_string)\r\n",
							"split_string = modified_string.split('}{')\r\n",
							"print(split_string)"
						],
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"list_of_dicts = []\r\n",
							"for string_elem in split_string:\r\n",
							"    # Assuming the strings represent key-value pairs separated by commas\r\n",
							"    dict_elem = dict(item.split(':') for item in string_elem.split(','))\r\n",
							"    list_of_dicts.append(dict_elem)\r\n",
							"\r\n",
							"# Display the list of dictionaries\r\n",
							"#for d in list_of_dicts:\r\n",
							"    #print(d)\r\n",
							"\r\n",
							"#cleaned_list_of_dicts = [{k.strip('\"'): v.strip('\"') for k, v in d.items()} for d in list_of_dicts]\r\n",
							"cleaned_list_of_dicts = []\r\n",
							"for d in list_of_dicts:\r\n",
							"    cleaned_dict = {re.sub(r'[\"\\']', '', k): re.sub(r'[\"\\']', '', v) for k, v in d.items()}\r\n",
							"    cleaned_list_of_dicts.append(cleaned_dict)\r\n",
							"for i in cleaned_list_of_dicts:\r\n",
							"    print(i)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 91
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = spark.createDataFrame(cleaned_list_of_dicts)\r\n",
							"\r\n",
							"# Show the DataFrame\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 92
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import json\r\n",
							"my_rdd = json_rdd.map(lambda x: json.dumps(x))"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"my_rdd.collect()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"json_rdd.collect()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"json_rdd = my_rdd.flatMap(lambda line: line.split('},')).map(lambda x: x.replace(\"[\", \"\").replace(\"]\", \"\").strip())\r\n",
							"\r\n",
							"# Parse each JSON string into a dictionary\r\n",
							"dict_rdd = json_rdd.map(lambda x: json.loads(x + (\"}\" if x.count(\"{\") != x.count(\"}\") else \"\")))\r\n",
							"\r\n",
							"# Convert RDD of dictionaries to DataFrame\r\n",
							"df = dict_rdd.toDF()\r\n",
							"\r\n",
							"# Show the DataFrame\r\n",
							"df.show(truncate=False)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"json_string = ''.join(my_rdd.flatMap(lambda x: x).collect())\r\n",
							"\r\n",
							"# Split the single string into separate JSON strings\r\n",
							"split_json_strings = json_string.split('},')\r\n",
							"\r\n",
							"# Add back the '}' to complete the JSON objects\r\n",
							"split_json_strings = [json_obj + ('}' if json_obj.count(\"{\") != json_obj.count(\"}\") else \"\") for json_obj in split_json_strings]\r\n",
							"\r\n",
							"# Parse each JSON string into a dictionary\r\n",
							"dict_rdd = spark.sparkContext.parallelize(split_json_strings).map(lambda x: json.loads(x))\r\n",
							"\r\n",
							"# Convert RDD of dictionaries to DataFrame\r\n",
							"df = dict_rdd.toDF()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"data = json_rdd.collect()\r\n",
							"cleaned_data = ' '.join(line.strip() for line in data if line not in ['[{', '{', '}', '}]'])\r\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(cleaned_data)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"split_data = cleaned_data.split('}, ')\r\n",
							"\r\n",
							"# Add back the '}' to complete the JSON objects\r\n",
							"split_data = [obj + ('}' if obj.count(\"{\") != obj.count(\"}\") else \"\") for obj in split_data]\r\n",
							"\r\n",
							"# Combine the separated objects into a list of dictionaries\r\n",
							"list_of_dicts = [json.loads(\"{\" + obj) for obj in split_data]\r\n",
							"\r\n",
							"# Display the list of dictionaries\r\n",
							"for d in list_of_dicts:\r\n",
							"    print(d)"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"result_list = []\r\n",
							"result_list = cleaned_data.split(',')\r\n",
							"print(result_list)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sublists = [result_list[i:i+5] for i in range(0, len(data), 5)]\r\n",
							"\r\n",
							"# Remove the trailing '}' in each sublist\r\n",
							"result = [sublist[:-1] for sublist in sublists]\r\n",
							"result.pop()\r\n",
							"result.pop()\r\n",
							"\r\n",
							"# Display the resulting list\r\n",
							"print(result)"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"data_tuples = [tuple(sublist) for sublist in result]\r\n",
							"\r\n",
							"# Create DataFrame\r\n",
							"columns = ['even_id', 'record_type', 'record_id', 'date']  # Assuming column names\r\n",
							"df = spark.createDataFrame(data_tuples, schema=columns)\r\n",
							"\r\n",
							"# Show the DataFrame\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rdd = spark.sparkContext.parallelize(result)\r\n",
							"rdd.take(1)"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rdd = spark.sparkContext.parallelize(result)\r\n",
							"def split_record(record):\r\n",
							"    return record[0].split(',')\r\n",
							"\r\n",
							"split_rdd = rdd.map(split_record)\r\n",
							"split_rdd.collect()"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"import json\r\n",
							"\r\n",
							"# Create a Spark session\r\n",
							"spark = SparkSession.builder.appName(\"OptimizedConversion\").getOrCreate()\r\n",
							"\r\n",
							"# Sample file path\r\n",
							"file_path = \"abfss://historical@myadls8434.dfs.core.windows.net/new/data_master.json\"\r\n",
							"\r\n",
							"# Read JSON file into a DataFrame directly\r\n",
							"df = spark.read.json(file_path)\r\n",
							"\r\n",
							"# Show the DataFrame\r\n",
							"df.show()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 93
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "mypool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "49911612-fcd7-4c23-ab70-d4853e8c959d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7374728e-d82a-4420-8e42-c2f7ef16e4c3/resourceGroups/mygrp/providers/Microsoft.Synapse/workspaces/workspace7971/bigDataPools/mypool",
						"name": "mypool",
						"type": "Spark",
						"endpoint": "https://workspace7971.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/mypool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import re"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import re\r\n",
							"file_path = f\"abfss://historical@myadls8434.dfs.core.windows.net/new/data_master.json\"\r\n",
							"# Read JSON file into an RDD\r\n",
							"json_rdd = sc.textFile(file_path)\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"# Combine JSON strings into a single JSON object or array\r\n",
							"# If an array is needed:\r\n",
							"json_array = \"[\" + ','.join(json_rdd.collect()) + \"]\"\r\n",
							"#print(json_array)\r\n",
							"modified_string = json_array[4:-4]\r\n",
							"modified_string = modified_string.replace(',},,{,', '}{')\r\n",
							"modified_string = modified_string.replace(',,', ',')\r\n",
							"#modified_string = '{'+modified_string+'}'\r\n",
							"#print(modified_string)\r\n",
							"split_string = modified_string.split('}{')\r\n",
							"#print(split_string)\r\n",
							"\r\n",
							"list_of_dicts = []\r\n",
							"for string_elem in split_string:\r\n",
							"    # Assuming the strings represent key-value pairs separated by commas\r\n",
							"    dict_elem = dict(item.split(':') for item in string_elem.split(','))\r\n",
							"    list_of_dicts.append(dict_elem)\r\n",
							"\r\n",
							"# Display the list of dictionaries\r\n",
							"#for d in list_of_dicts:\r\n",
							"    #print(d)\r\n",
							"\r\n",
							"#cleaned_list_of_dicts = [{k.strip('\"'): v.strip('\"') for k, v in d.items()} for d in list_of_dicts]\r\n",
							"cleaned_list_of_dicts = []\r\n",
							"for d in list_of_dicts:\r\n",
							"    cleaned_dict = {re.sub(r'[\"\\']', '', k): re.sub(r'[\"\\']', '', v) for k, v in d.items()}\r\n",
							"    cleaned_list_of_dicts.append(cleaned_dict)\r\n",
							"#for i in cleaned_list_of_dicts:\r\n",
							"    #print(i)\r\n",
							"\r\n",
							"df = spark.createDataFrame(cleaned_list_of_dicts)\r\n",
							"\r\n",
							"# Show the DataFrame\r\n",
							"df.show()\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/mypool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": []
			},
			"dependsOn": []
		}
	]
}