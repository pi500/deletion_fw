{
	"name": "Notebook 1",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "mypool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "0583b709-2f33-41f0-9b41-f63e61b567a7"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7374728e-d82a-4420-8e42-c2f7ef16e4c3/resourceGroups/mygrp/providers/Microsoft.Synapse/workspaces/workspace7971/bigDataPools/mypool",
				"name": "mypool",
				"type": "Spark",
				"endpoint": "https://workspace7971.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/mypool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"#loc = f'abfss://historical@myadls8434/new/{file_name}'\r\n",
					"loc = f'abfss://historical@myadls8434/new/data_master.json'\r\n",
					"print(loc)\r\n",
					""
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"\r\n",
					"adls_path = \"adl://myadls8434.azuredatalakestore.net/historical/new/your/data_master.json\"\r\n",
					"#loc = f'abfss://historical@myadls8434/new/data_master.json'\r\n",
					"# Read JSON file into a DataFrame\r\n",
					"df = spark.read.json(adls_path)\r\n",
					"\r\n",
					"# Show DataFrame schema and some sample data\r\n",
					"df.printSchema()\r\n",
					"df.show()\r\n",
					""
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Import necessary libraries\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"\r\n",
					"# Create a Spark session\r\n",
					"\r\n",
					"\r\n",
					"# Define the path to your JSON file in Azure Storage\r\n",
					"file_path = \"abfss://historical@myadls8434.dfs.core.windows.net/new/delete_master.json\"\r\n",
					"\r\n",
					"# Read JSON file into DataFrame\r\n",
					"df = spark.read.json(file_path)\r\n",
					"\r\n",
					"# Show DataFrame schema and some data\r\n",
					"df.printSchema()\r\n",
					"df.show()\r\n",
					""
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.types import *\r\n",
					"schema1 = StructType([\\\r\n",
					"    StructField('data_node',StringType()),\\\r\n",
					"    StructField('data_product',StringType()),\\\r\n",
					"    StructField('data_table',StringType()),\\\r\n",
					"    StructField('record_type',StringType()),\\\r\n",
					"    StructField('container',StringType())])\r\n",
					"file_path = \"abfss://historical@myadls8434.dfs.core.windows.net/new/data_master.json\"\r\n",
					"data = [\r\n",
					"\t{\r\n",
					"\"data_node\" : \"IPN\",\r\n",
					"\"data_product\" : \"InvolvedParty\",\r\n",
					"\"data_table\" : \"PERSON\",\r\n",
					"\"record_type\" : \"Customer\",\r\n",
					"\"container\" : \"historical\"\r\n",
					"}\r\n",
					"]\r\n",
					"# Read JSON file into DataFrame\r\n",
					"#df = spark.read.json(file_path)\r\n",
					"df= spark.createDataFrame(data,schema1)\r\n",
					"\r\n",
					"# Show DataFrame schema and some data\r\n",
					"df.printSchema()\r\n",
					"df.show()"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"file_path = \"abfss://historical@myadls8434.dfs.core.windows.net/new/data_master.json\"\r\n",
					"# Read JSON file into an RDD\r\n",
					"json_rdd = sc.textFile(file_path)\r\n",
					"\r\n",
					"# Show some content from the RDD\r\n",
					"json_rdd.collect()\r\n",
					""
				],
				"execution_count": 89
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"son_rdd = json_rdd.map(lambda x: json.dumps(x))\r\n",
					"\r\n",
					"# Combine JSON strings into a single JSON object or array\r\n",
					"# If an array is needed:\r\n",
					"json_array = \"[\" + ','.join(json_rdd.collect()) + \"]\"\r\n",
					"#print(json_array)\r\n",
					"modified_string = json_array[4:-4]\r\n",
					"modified_string = modified_string.replace(',},,{,', '}{')\r\n",
					"modified_string = modified_string.replace(',,', ',')\r\n",
					"#modified_string = '{'+modified_string+'}'\r\n",
					"print(modified_string)\r\n",
					"split_string = modified_string.split('}{')\r\n",
					"print(split_string)"
				],
				"execution_count": 90
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"list_of_dicts = []\r\n",
					"for string_elem in split_string:\r\n",
					"    # Assuming the strings represent key-value pairs separated by commas\r\n",
					"    dict_elem = dict(item.split(':') for item in string_elem.split(','))\r\n",
					"    list_of_dicts.append(dict_elem)\r\n",
					"\r\n",
					"# Display the list of dictionaries\r\n",
					"#for d in list_of_dicts:\r\n",
					"    #print(d)\r\n",
					"\r\n",
					"#cleaned_list_of_dicts = [{k.strip('\"'): v.strip('\"') for k, v in d.items()} for d in list_of_dicts]\r\n",
					"cleaned_list_of_dicts = []\r\n",
					"for d in list_of_dicts:\r\n",
					"    cleaned_dict = {re.sub(r'[\"\\']', '', k): re.sub(r'[\"\\']', '', v) for k, v in d.items()}\r\n",
					"    cleaned_list_of_dicts.append(cleaned_dict)\r\n",
					"for i in cleaned_list_of_dicts:\r\n",
					"    print(i)\r\n",
					""
				],
				"execution_count": 91
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df = spark.createDataFrame(cleaned_list_of_dicts)\r\n",
					"\r\n",
					"# Show the DataFrame\r\n",
					"df.show()"
				],
				"execution_count": 92
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import json\r\n",
					"my_rdd = json_rdd.map(lambda x: json.dumps(x))"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"my_rdd.collect()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"json_rdd.collect()"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"json_rdd = my_rdd.flatMap(lambda line: line.split('},')).map(lambda x: x.replace(\"[\", \"\").replace(\"]\", \"\").strip())\r\n",
					"\r\n",
					"# Parse each JSON string into a dictionary\r\n",
					"dict_rdd = json_rdd.map(lambda x: json.loads(x + (\"}\" if x.count(\"{\") != x.count(\"}\") else \"\")))\r\n",
					"\r\n",
					"# Convert RDD of dictionaries to DataFrame\r\n",
					"df = dict_rdd.toDF()\r\n",
					"\r\n",
					"# Show the DataFrame\r\n",
					"df.show(truncate=False)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"json_string = ''.join(my_rdd.flatMap(lambda x: x).collect())\r\n",
					"\r\n",
					"# Split the single string into separate JSON strings\r\n",
					"split_json_strings = json_string.split('},')\r\n",
					"\r\n",
					"# Add back the '}' to complete the JSON objects\r\n",
					"split_json_strings = [json_obj + ('}' if json_obj.count(\"{\") != json_obj.count(\"}\") else \"\") for json_obj in split_json_strings]\r\n",
					"\r\n",
					"# Parse each JSON string into a dictionary\r\n",
					"dict_rdd = spark.sparkContext.parallelize(split_json_strings).map(lambda x: json.loads(x))\r\n",
					"\r\n",
					"# Convert RDD of dictionaries to DataFrame\r\n",
					"df = dict_rdd.toDF()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"data = json_rdd.collect()\r\n",
					"cleaned_data = ' '.join(line.strip() for line in data if line not in ['[{', '{', '}', '}]'])\r\n",
					""
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"print(cleaned_data)"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"split_data = cleaned_data.split('}, ')\r\n",
					"\r\n",
					"# Add back the '}' to complete the JSON objects\r\n",
					"split_data = [obj + ('}' if obj.count(\"{\") != obj.count(\"}\") else \"\") for obj in split_data]\r\n",
					"\r\n",
					"# Combine the separated objects into a list of dictionaries\r\n",
					"list_of_dicts = [json.loads(\"{\" + obj) for obj in split_data]\r\n",
					"\r\n",
					"# Display the list of dictionaries\r\n",
					"for d in list_of_dicts:\r\n",
					"    print(d)"
				],
				"execution_count": 40
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"result_list = []\r\n",
					"result_list = cleaned_data.split(',')\r\n",
					"print(result_list)"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"sublists = [result_list[i:i+5] for i in range(0, len(data), 5)]\r\n",
					"\r\n",
					"# Remove the trailing '}' in each sublist\r\n",
					"result = [sublist[:-1] for sublist in sublists]\r\n",
					"result.pop()\r\n",
					"result.pop()\r\n",
					"\r\n",
					"# Display the resulting list\r\n",
					"print(result)"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"data_tuples = [tuple(sublist) for sublist in result]\r\n",
					"\r\n",
					"# Create DataFrame\r\n",
					"columns = ['even_id', 'record_type', 'record_id', 'date']  # Assuming column names\r\n",
					"df = spark.createDataFrame(data_tuples, schema=columns)\r\n",
					"\r\n",
					"# Show the DataFrame\r\n",
					"df.show()"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"rdd = spark.sparkContext.parallelize(result)\r\n",
					"rdd.take(1)"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"rdd = spark.sparkContext.parallelize(result)\r\n",
					"def split_record(record):\r\n",
					"    return record[0].split(',')\r\n",
					"\r\n",
					"split_rdd = rdd.map(split_record)\r\n",
					"split_rdd.collect()"
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"import json\r\n",
					"\r\n",
					"# Create a Spark session\r\n",
					"spark = SparkSession.builder.appName(\"OptimizedConversion\").getOrCreate()\r\n",
					"\r\n",
					"# Sample file path\r\n",
					"file_path = \"abfss://historical@myadls8434.dfs.core.windows.net/new/data_master.json\"\r\n",
					"\r\n",
					"# Read JSON file into a DataFrame directly\r\n",
					"df = spark.read.json(file_path)\r\n",
					"\r\n",
					"# Show the DataFrame\r\n",
					"df.show()\r\n",
					""
				],
				"execution_count": 93
			}
		]
	}
}