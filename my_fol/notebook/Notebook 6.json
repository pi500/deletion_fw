{
	"name": "Notebook 6",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "mypool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "fe4a1861-f8a5-481a-b474-53bab1c6d6df"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7374728e-d82a-4420-8e42-c2f7ef16e4c3/resourceGroups/ind_grp/providers/Microsoft.Synapse/workspaces/myworkspace7971/bigDataPools/mypool",
				"name": "mypool",
				"type": "Spark",
				"endpoint": "https://myworkspace7971.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/mypool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%pip install azure-data-tables==12.5.0"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"from azure.core.credentials import AzureNamedKeyCredential\r\n",
					"from azure.data.tables import TableServiceClient\r\n",
					"from azure.data.tables import TableClient\r\n",
					"from datetime import datetime\r\n",
					"import pytz\r\n",
					"\r\n",
					"\r\n",
					"# Replace these with your Azure Table Storage credentials\r\n",
					"account_name = 'newadls8434'\r\n",
					"account_key = 'C2wgRF7Xn43ECMRQ29X+sL3PdunLo2xRK5jBKuG17ViVSzSSX7JrbIt7f6zAK0ePW81qiHAk1Iec+ASt82yl/w=='\r\n",
					"new_table = 'newtable123'\r\n",
					"my_table = 'mytable123'\r\n",
					"lat_table = 'lat123table'\r\n",
					"entity_list = []\r\n",
					"df_list = []\r\n",
					"filter_condition = \"status eq 'not found'\"\r\n",
					"update_condition = {\"name\":\"shishir\",\"status\":\"to be deleted\"}\r\n",
					"\r\n",
					"# Create a table service object\r\n",
					"endpoint = f\"https://{account_name}.table.core.windows.net/\"\r\n",
					"credential = AzureNamedKeyCredential(account_name, account_key)\r\n",
					"table_service = TableServiceClient(endpoint=endpoint, credential=credential)\r\n",
					"\r\n",
					"#current_timestamp = datetime.timestamp(datetime.now())\r\n",
					"current_timestamp= datetime.now(pytz.timezone('Asia/Kolkata')).strftime('%Y-%m-%d %I:%M %p')\r\n",
					"print(current_timestamp)\r\n",
					"entiity = [{\r\n",
					"    'PartitionKey': '20',\r\n",
					"    'RowKey': '4',\r\n",
					"    'name': 'rutuja',\r\n",
					"    'stamp' : str(current_timestamp),\r\n",
					"    'status' : 'not found'\r\n",
					"},\r\n",
					"{\r\n",
					"    'PartitionKey': '10',\r\n",
					"    'RowKey': '12',\r\n",
					"    'name': 'piyush',\r\n",
					"    'stamp' : str(current_timestamp),\r\n",
					"    'status' : 'not found'\r\n",
					"},{\r\n",
					"    'PartitionKey': '30',\r\n",
					"    'RowKey': '6',\r\n",
					"    'name': 'yash',\r\n",
					"    'stamp' : str(current_timestamp),\r\n",
					"    'status' : 'found'\r\n",
					"}]\r\n",
					"\r\n",
					"print(entiity)\r\n",
					"def create_table(table_name):\r\n",
					"    table_client = table_service.create_table(table_name=table_name)\r\n",
					"def insert_entity1(entiity, table_name):\r\n",
					"    print(entiity)\r\n",
					"    table_client = table_service.get_table_client(table_name)\r\n",
					"    for i in entity:\r\n",
					"        table_client.upsert_entity(i)\r\n",
					"\r\n",
					"def convert_to_empty_df1(table_name):\r\n",
					"    table_client = table_service.get_table_client(table_name)\r\n",
					"    all_entities = table_client.list_entities()\r\n",
					"\r\n",
					"    # Extract and print the properties (column names) from the sample entities\r\n",
					"    if all_entities:\r\n",
					"        properties = set()\r\n",
					"        for entity in all_entities:\r\n",
					"            properties.update(entity.keys())\r\n",
					"\r\n",
					"        print(\"Properties (Column Names):\")\r\n",
					"        print(properties)\r\n",
					"    else:\r\n",
					"        print(\"No entities found in the table.\")\r\n",
					"\r\n",
					"    from pyspark.sql.types import StructType, StructField, StringType\r\n",
					"    my_list = list(properties)\r\n",
					"    print(my_list)\r\n",
					"    schema = StructType([\r\n",
					"        StructField(column, StringType(), True) for column in my_list\r\n",
					"    ])\r\n",
					"    empty_df = spark.createDataFrame([], schema)\r\n",
					"    empty_df.show()\r\n",
					"\r\n",
					"def table_to_df1(table_name):\r\n",
					"    table_client = table_service.get_table_client(table_name)\r\n",
					"    all_entities = table_client.list_entities()\r\n",
					"        \r\n",
					"    entity_list = []\r\n",
					"    # Print or process the retrieved entities\r\n",
					"    for entity in all_entities:\r\n",
					"        print(f'all entities: {entity}')\r\n",
					"        entity_dict = dict(entity)  # Convert Entity object to a dictionary\r\n",
					"        print(entity_dict)\r\n",
					"        entity_list.append(entity_dict)\r\n",
					"    print(f'entity_list:{entity_list}')\r\n",
					"    df = spark.createDataFrame(entity_list)\r\n",
					"\r\n",
					"    # Show the DataFrame\r\n",
					"    df.show()\r\n",
					"    return df\r\n",
					"\r\n",
					"new_list = []\r\n",
					"'''\r\n",
					"#filter_condition = \"status eq 'to be deleted'\"\r\n",
					"def query_table1(table_name, filter_condition):\r\n",
					"    table_client = table_service.get_table_client(table_name)\r\n",
					"    # Execute the query\r\n",
					"    query_result = table_client.query_entities(query_filter=filter_condition)\r\n",
					"    print(f'query_result:{query_result}')\r\n",
					"    for i in query_result:\r\n",
					"        print('i is:', i)\r\n",
					"        new_list.append(i)\r\n",
					"        print(f'new_list:{new_list}')\r\n",
					"        \r\n",
					"        df = spark.createDataFrame(query_result)\r\n",
					"        df.show(truncate=False)'''\r\n",
					"def query_table1(table_name, filter_condition):\r\n",
					"    try:\r\n",
					"        print('1')\r\n",
					"        create_table(table_name)\r\n",
					"\r\n",
					"    except:\r\n",
					"        print('2')\r\n",
					"        table_client = table_service.get_table_client(table_name)\r\n",
					"    # Execute the query with pagination\r\n",
					"    query_result = table_client.query_entities(query_filter=filter_condition)\r\n",
					"    \r\n",
					"    # Process each page of results\r\n",
					"    for page in query_result.by_page():\r\n",
					"        for entity in page:\r\n",
					"            print('Entity:', entity)\r\n",
					"            new_list.append(entity)\r\n",
					"    print(new_list)\r\n",
					"    # Create DataFrame from the collected entities\r\n",
					"    df = spark.createDataFrame(new_list)\r\n",
					"    df.show(truncate=False)\r\n",
					"\r\n",
					"def update_table1(df, table_name, col_name,update_condition):\r\n",
					"    table_client = table_service.get_table_client(table_name)\r\n",
					"\r\n",
					"    # Convert PySpark DataFrame to a list of dictionaries\r\n",
					"    data_to_insert = df.collect()\r\n",
					"    list_of_entities = []\r\n",
					"    print(f'data_insert:{data_to_insert}')\r\n",
					"\r\n",
					"    for row in data_to_insert:\r\n",
					"        # Assuming your DataFrame columns match your Azure Table Storage columns\r\n",
					"        print(row)\r\n",
					"        entity = {\r\n",
					"            'PartitionKey': row['PartitionKey'],\r\n",
					"            'RowKey': row['RowKey'],\r\n",
					"            'stamp': row['stamp'],\r\n",
					"            'name': row['name'],\r\n",
					"            'status': row['status']\r\n",
					"            # Add more columns as needed\r\n",
					"        }\r\n",
					"        list_of_entities.append(entity)\r\n",
					"        print(list_of_entities)\r\n",
					"\r\n",
					"    my_lis = []\r\n",
					"    for row in df.collect():\r\n",
					"        #partition_key_value = row['PartitionKey']  # Assuming this column exists in df\r\n",
					"        row_key_value = row[f'{col_name}']\r\n",
					"        print(row_key_value)\r\n",
					"        my_lis.append(row_key_value)\r\n",
					"\r\n",
					"    for i in my_lis:\r\n",
					"        filter_condition2 = f\"{col_name} eq '{i}'\"\r\n",
					"\r\n",
					"        # Retrieve entities that match the filter condition\r\n",
					"        entities_to_update = table_client.query_entities(query_filter=filter_condition2)\r\n",
					"\r\n",
					"        # Update the desired column in the matching entities\r\n",
					"        for entity in entities_to_update:\r\n",
					"            for k,v in update_condition.items():\r\n",
					"                entity[f'{k}'] = v\r\n",
					"                table_client.update_entity(entity)\r\n",
					""
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"query_table1(my_table,filter_condition)"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}